<h1 id="gmm算法"><strong>GMM算法</strong></h1>

<p>参考：<a href="http://blog.pluskid.org/?p=39">漫谈 Clustering (3): Gaussian Mixture Model</a></p>

<h2 id="什么是高斯混合模型">什么是高斯混合模型？</h2>

<p>高斯分布相信大家都不陌生，就是那个钟性的曲线。 <br>
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/360px-Normal_Distribution_PDF.svg.png" alt="wiki高斯分布图" title=""></p>

<p>那么何谓高斯分布模型呢？其实就犹如其名字所表达的那样，就是由多个高斯曲线叠加而成的一个模型。 <br>
举个例子吧，就上图中的蓝色和绿色的高斯曲线吧。 <br>
蓝色和绿色的高斯曲线组合在一起就是一个高斯混合模型了，如果将其看成是分类或者是聚类的话，可以这样理解：</p>

<blockquote>
  <p>其中有两个类别分别为蓝色和绿色，其数据可以想象成下面图片所示，蓝色看成是那个又小又密的点集，而绿色看成是那个类似长条形的数据集。（下面的那种图是从scikit-learn上扣下来的，不要能完全对于高斯曲线，意思对就行了。）</p>
  
  <p>如果在图中有一个点，可以分别计算这个点在两个高斯模型下的概率，如果在绿色高斯下的概率大于蓝色的高斯概率，那么这个点就属于绿色阵营了。其基本思想就是这样的了。</p>
</blockquote>

<p><img src="http://scikit-learn.org/stable/_images/sphx_glr_plot_gmm_pdf_0011.png" alt="不是太规范的例子" title=""></p>

<p><strong>总结：</strong> <br>
高斯混合模型就是由多个高斯曲线组合而成的模型。 <br>
如果有一个数据点需要判断其类别，将每一条高斯曲线看成是一个类别，只需要计算此点分别在每个高斯曲线下的概率，将此点划分到概率高的类别中。</p>



<h2 id="如何求解高斯混合模型">如何求解高斯混合模型</h2>

<p>数据 =&gt; 模型</p>

<p>每个GMM都是由k个Gaussian 分布组成的，每个Gaussian称为一个“Component”，这些“Component”线性加成在一起就组成了GMM的概率密度函数：</p>



<p><script type="math/tex; mode=display" id="MathJax-Element-2308">p(x)=\sum_{k=1}^{k}p(x) \cdot p(x|y) =\sum_{k=1}^{k}\pi_k N(x|u_k ,\Sigma_k)</script></p>

<p>从中可以看成需要确定这个模型需要确定如下参数：<script type="math/tex" id="MathJax-Element-2309">\pi_k , \mu_k , \Sigma_k</script></p>

<p><strong>ps：发现写漫谈系列的文章真心不错，下面就照搬吧！！</strong></p>

<p>现在假设我们有 N 个数据点，并假设它们服从某个分布（记作 p(x) ），现在要确定里面的一些参数的值，例如，在 GMM 中，我们就需要确定 <script type="math/tex" id="MathJax-Element-2310">\pi_k</script>、<script type="math/tex" id="MathJax-Element-2311">\mu_k</script> 和 <script type="math/tex" id="MathJax-Element-2312">\Sigma_k</script> 这些参数。 我们的想法是，找到这样一组参数，它所确定的概率分布生成这些给定的数据点的概率最大，而这个概率实际上就等于 <script type="math/tex" id="MathJax-Element-2313">\prod_{i=1}^N p(x_i)</script> ，我们把这个乘积称作似然函数 (Likelihood Function)。通常单个点的概率都很小，许多很小的数字相乘起来在计算机里很容易造成浮点数下溢，因此我们通常会对其取对数，把乘积变成加和 <script type="math/tex" id="MathJax-Element-2314">\sum_{i=1}^N \log p(x_i)</script>，得到 log-likelihood function 。接下来我们只要将这个函数最大化（通常的做法是求导并令导数等于零，然后解方程），亦即找到这样一组参数值，它让似然函数取得最大值，我们就认为这是最合适的参数，这样就完成了参数估计的过程。</p>

<p>GMM的 log-likelihood function 如下：</p>

<p><script type="math/tex; mode=display" id="MathJax-Element-3046">\sum_{i=1}^{n}\log\{\sum_{k=1}^{k} \pi_k N(x_i | \mu_k , \Sigma_k ) \}</script></p>

<p>由于在对数函数里面又有加和，我们没法直接用求导解方程的办法直接求得最大值。为了解决这个问题，我们采取之前从 GMM 中随机选点的办法：分成两步，实际上也就类似于 K-means 的两步。</p>

<blockquote>
  <ol>
  <li><strong>估计数据由每个 Component 生成的概率（并不是每个 Component 被选中的概率）：</strong> <br>
  对于每个数据 <script type="math/tex" id="MathJax-Element-3047">x_i</script> 来说，它由第 k 个 Component 生成的概率为 <br>
  <script type="math/tex; mode=display" id="MathJax-Element-3048">\gamma(i, k) = \frac{\pi_k \mathcal{N}(x_i|\mu_k, \Sigma_k)}{\sum_{j=1}^K \pi_j\mathcal{N}(x_i|\mu_j, \Sigma_j)}</script></li>
  </ol>
  
  <p>由于式子里的 <script type="math/tex" id="MathJax-Element-3049">\mu_k</script> 和 <script type="math/tex" id="MathJax-Element-3050">\Sigma_k</script> 也是需要我们估计的值，我们采用迭代法，在计算<script type="math/tex" id="MathJax-Element-3051"> \gamma(i, k)</script> 的时候我们假定 <script type="math/tex" id="MathJax-Element-3052">\mu_k</script> 和 <script type="math/tex" id="MathJax-Element-3053">\Sigma_k</script> 均已知，我们将取上一次迭代所得的值（或者初始值）。 <br>
  2. <strong>估计每个 Component 的参数： </strong> <br>
  现在我们假设上一步中得到的 <script type="math/tex" id="MathJax-Element-3054">\gamma(i, k)</script> 就是正确的“数据 <script type="math/tex" id="MathJax-Element-3055">x_i </script>由 Component k 生成的概率”，亦可以当做该 Component 在生成这个数据上所做的贡献，或者说，我们可以看作<script type="math/tex" id="MathJax-Element-3056"> x_i</script> 这个值其中有 <script type="math/tex" id="MathJax-Element-3057">\gamma(i, k)x_i </script>这部分是由 Component k 所生成的。集中考虑所有的数据点，现在实际上可以看作 Component 生成了<script type="math/tex" id="MathJax-Element-3058"> \gamma(1, k)x_1, \ldots, \gamma(N, k)x_N</script> 这些点。由于每个 Component 都是一个标准的 Gaussian 分布，可以很容易分布求出最大似然所对应的参数值： <br>
  <script type="math/tex; mode=display" id="MathJax-Element-3059">
\begin{aligned}
\mu_k & = \frac{1}{N_k}\sum_{i=1}^N\gamma(i, k)x_i \\
\Sigma_k & = \frac{1}{N_k}\sum_{i=1}^N\gamma(i,
k)(x_i-\mu_k)(x_i-\mu_k)^T
\end{aligned}</script> <br>
  其中 <script type="math/tex" id="MathJax-Element-3060">N_k = \sum_{i=1}^N \gamma(i, k)</script> ，并且 <script type="math/tex" id="MathJax-Element-3061">\pi_k</script> 也顺理成章地可以估计为 <script type="math/tex" id="MathJax-Element-3062">N_k/N</script> 。 <br>
  3. <strong>重复迭代前面两步，直到似然函数的值收敛为止。</strong></p>
</blockquote>



<h2 id="源码">源码：</h2>

<p>python版本是我自己实现的，matlab版本是漫谈系列中的。</p>

<p><a href="http://blog.pluskid.org/?p=39">matlab版本</a> <br>
<a href="https://github.com/jie147/myMachineLearningLib/blob/master/com.jie/Unsupervised/GMM.py">python版本</a></p>



<pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment"># -*- coding: UTF-8 -*-</span>
<span class="hljs-comment"># !/usr/bin/python</span>
<span class="hljs-keyword">import</span> random

<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> numpy <span class="hljs-keyword">import</span> zeros, array, tile, mean, cov, shape, repeat, sqrt
<span class="hljs-keyword">import</span> numpy.matlib <span class="hljs-keyword">as</span> ml
<span class="hljs-keyword">from</span> numpy.linalg <span class="hljs-keyword">import</span> det, inv


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">GMM</span><span class="hljs-params">(X, k, threshold=<span class="hljs-number">1e-15</span>)</span>:</span>
    n, m = X.shape
    <span class="hljs-comment"># 挑选个中心点</span>
    centers = array(random.sample(X, k))

    <span class="hljs-comment"># 初始化 miu ， pi ， sigma</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">init_param</span><span class="hljs-params">()</span>:</span>
        <span class="hljs-comment"># 计算数据点归属那类</span>
        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">distance</span><span class="hljs-params">(X, Y)</span>:</span>
            n = len(X)
            m = len(Y)
            xx = ml.sum(X * X, axis=<span class="hljs-number">1</span>)
            yy = ml.sum(Y * Y, axis=<span class="hljs-number">1</span>)
            xy = ml.dot(X, Y.T)
            <span class="hljs-keyword">return</span> tile(xx, (m, <span class="hljs-number">1</span>)).T + tile(yy, (n, <span class="hljs-number">1</span>)) - <span class="hljs-number">2</span> * xy

        dist = distance(X, centers)
        label = dist.argmin(axis=<span class="hljs-number">1</span>)
        <span class="hljs-comment"># 求解初始 pi 和 sigma</span>
        init_pi = zeros((<span class="hljs-number">1</span>, k), dtype=np.float64)
        init_sigma = zeros((k, m, m), dtype=np.float64)
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(k):
            one_type = X[label == i]
            init_pi[:, i] = <span class="hljs-number">1.</span> * shape(one_type)[<span class="hljs-number">0</span>] / n
            init_sigma[i, :, :] = ml.cov(one_type.T)
        <span class="hljs-keyword">return</span> init_pi, init_sigma

    mui = centers
    pi, sigma = init_param()

    <span class="hljs-keyword">print</span> <span class="hljs-string">"the shape of init param: mui,pi,sigma"</span>
    <span class="hljs-keyword">print</span> mui.shape, <span class="hljs-string">"  "</span>, pi.shape, <span class="hljs-string">"  "</span>, sigma.shape
    <span class="hljs-keyword">print</span> <span class="hljs-string">"--"</span> * <span class="hljs-number">10</span>
    <span class="hljs-keyword">print</span> mui
    <span class="hljs-keyword">print</span> <span class="hljs-string">"--"</span> * <span class="hljs-number">10</span>
    <span class="hljs-keyword">print</span> pi
    <span class="hljs-keyword">print</span> <span class="hljs-string">"--"</span> * <span class="hljs-number">10</span>
    <span class="hljs-keyword">print</span> sigma
    <span class="hljs-keyword">print</span> <span class="hljs-string">"--"</span> * <span class="hljs-number">10</span>

    <span class="hljs-comment"># 计算每个点在每个模型中的概率</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">calc_pro</span><span class="hljs-params">(tmp_mui, tmp_sigma)</span>:</span>
        p_x = zeros((n, k))
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(k):
            px_u = X - tmp_mui[i, :]
            <span class="hljs-comment"># print px_u</span>
            inv_sigma = inv(tmp_sigma[i, :, :])
            <span class="hljs-comment"># print inv_sigma</span>
            tmp = ml.sum(px_u.dot(inv_sigma) * px_u, axis=<span class="hljs-number">1</span>)
            coef = (<span class="hljs-number">2</span> * <span class="hljs-number">3.1415926</span> * det(inv_sigma)) ** (-<span class="hljs-number">1</span> / <span class="hljs-number">2</span>)
            p_x[:, i] = coef * (ml.exp(-<span class="hljs-number">0.5</span> * tmp))
        <span class="hljs-keyword">return</span> p_x

    L_pre = float(<span class="hljs-string">'-Inf'</span>)
    loop = <span class="hljs-number">0</span>
    <span class="hljs-keyword">while</span> <span class="hljs-keyword">True</span>:
        loop += <span class="hljs-number">1</span>
        <span class="hljs-keyword">print</span> <span class="hljs-string">"the number of compute "</span>, loop
        Px = calc_pro(mui, sigma)

        <span class="hljs-comment"># print Px</span>
        gamma = Px * pi
        gamma = gamma / ml.sum(gamma, axis=<span class="hljs-number">0</span>)

        Nk = ml.sum(gamma, axis=<span class="hljs-number">0</span>)
        mui = gamma.T.dot(X)
        mui = ml.diag(<span class="hljs-number">1</span> / Nk).dot(mui)
        pi = Nk / n
        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(k):
            x_u = X - mui[j, :]
            sigma[j, :, :] = x_u.T.dot(ml.diag(gamma[:, j]).dot(x_u)) / Nk[j]

        L = ml.sum(ml.log(Px.dot(pi.T)))
        <span class="hljs-keyword">print</span> <span class="hljs-string">"the L is :"</span>,L
        <span class="hljs-keyword">print</span> <span class="hljs-string">"the diff of L is "</span>, L - L_pre
        <span class="hljs-keyword">if</span> (L - L_pre &lt; threshold):
            <span class="hljs-keyword">break</span>
        <span class="hljs-comment"># if (loop &gt; 300):</span>
        <span class="hljs-comment">#     break</span>
        L_pre = L

    <span class="hljs-keyword">return</span> mui, sigma, pi, Px


<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    test_a = np.random.rand(<span class="hljs-number">250</span>)
    test_a.resize((<span class="hljs-number">50</span>, <span class="hljs-number">5</span>))
    <span class="hljs-keyword">print</span> test_a
    mui, sigma, pi, Px = GMM(test_a, <span class="hljs-number">2</span>)
    <span class="hljs-keyword">print</span> <span class="hljs-string">" the mui is :"</span>
    <span class="hljs-keyword">print</span> mui
    <span class="hljs-keyword">print</span> <span class="hljs-string">" the sigma is :"</span>
    <span class="hljs-keyword">print</span> sigma
    <span class="hljs-keyword">print</span> <span class="hljs-string">" the pi is :"</span>
    <span class="hljs-keyword">print</span> pi</code></pre>